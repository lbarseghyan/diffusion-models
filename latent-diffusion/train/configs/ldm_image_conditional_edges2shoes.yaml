# --- Dataset ---------------------------------------------
dataset:
  dataset_root: ../data/pix2pix/edges2shoes/train
  image_size: 32

# ─── VAE (VQModel) settings ───────────────────────────────────────────────────
ddconfig:
  double_z: false
  z_channels: 3
  resolution: 32
  in_channels: 3
  out_ch: 3
  ch: 64
  ch_mult: [1, 2]
  num_res_blocks: 2
  attn_resolutions: []
  dropout: 0.0

lossconfig:
  target: "taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator"
  params:
    disc_conditional: false
    disc_in_channels: 3
    disc_start: 0
    disc_weight: 0.75
    codebook_weight: 1.0

embed_dim: 3
n_embed: 8192
base_learning_rate: 4.5e-06

condition_checkpoint_path: "/home/user1807/Desktop/diffusion-models-archive/results/VAE/edges2shoes_cond_32_to_16/checkpoints/model_epoch=99.ckpt"
target_checkpoint_path: "/home/user1807/Desktop/diffusion-models-archive/results/VAE/edges2shoes_target_32_to_16/checkpoints/model_epoch=69.ckpt"

# ─── Diffusion settings ────────────────────────────────────────────────
latentdiffusion:
  timesteps: 1000

# ─── UNet settings ────────────────────────────────────────────────────────────
unet:
  dim: 64
  dim_mults: [1, 2, 4, 8]
  dropout: 0.1
  channels: 3
  cond_channels: 3

# ─── Trainer settings ─────────────────────────────────────────────────────────
trainer:
  dataset_root: "../data/pix2pix/edges2shoes/train"    # update with your dataset path       
  train_batch_size: 64
  train_lr: 2e-4
  train_num_steps: 100000
  save_and_sample_every: 10000
  num_fid_samples: 1000
  results_folder: "results/ldm/edges2shoes_latent_32_to_16"
  calculate_is: true
  calculate_fid: true