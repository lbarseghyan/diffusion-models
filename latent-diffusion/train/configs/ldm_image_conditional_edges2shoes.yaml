# --- Dataset ---------------------------------------------
dataset:
  dataset_root: ../data/pix2pix/edges2shoes/train
  image_size: 256

# ─── VAE (VQModel) settings ───────────────────────────────────────────────────
ddconfig:
  double_z: false
  z_channels: 3
  resolution: 256
  in_channels: 3
  out_ch: 3
  ch: 64
  ch_mult: [1, 2, 4, 8]
  num_res_blocks: 2
  attn_resolutions: []
  dropout: 0.0

lossconfig:
  target: "taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator"
  params:
    disc_conditional: false
    disc_in_channels: 3
    disc_start: 0
    disc_weight: 0.75
    codebook_weight: 1.0

embed_dim: 3
n_embed: 8192
base_learning_rate: 4.5e-06

condition_checkpoint_path: "/home/user1809/Desktop/vae_checkpoint/cond_vae.ckpt"
target_checkpoint_path: "/home/user1809/Desktop/vae_checkpoint/target_vae.ckpt"

# ─── Diffusion settings ────────────────────────────────────────────────
latentdiffusion:
  timesteps: 1000

# ─── UNet settings ────────────────────────────────────────────────────────────
unet:
  dim: 64
  dim_mults: [1, 2, 4, 8]
  dropout: 0.1
  channels: 3
  cond_channels: 3

# ─── Trainer settings ─────────────────────────────────────────────────────────
trainer:
  dataset_root: "../data/pix2pix/edges2shoes/train"    # update with your dataset path       
  train_batch_size: 16
  train_lr: 2e-4
  train_num_steps: 800000
  save_and_sample_every: 100
  num_fid_samples: 1000
  results_folder: "results/ldm/edges2shoes_latent_32_test"
  calculate_is: true
  calculate_fid: true